{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Data Cleaning and Preprocessing\n",
    "\n",
    "**Scenario:** The first batch of real-time data has arrived from the field. Intelligence reports that a solar storm occurred between 08:00 and 09:30 Zulu, potentially corrupting any data transmitted during that window. Your first task is to isolate and remove these untrustworthy reports. After that, you must develop a robust, reusable Python function to clean the remaining messy data.\n",
    "\n",
    "**Coding Task Overview:** You will load the messy data, convert the timestamp column to a datetime format, and then filter out the unreliable rows from the solar storm period. You will then develop a `clean_data` function to handle various errors like missing values, incorrect data types, and typos. Finally, you will save both your cleaned data to a new CSV and your reusable function to an external Python utility file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Load and Inspect the Raw Data\n",
    "\n",
    "STUDENT CODE REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the field_reports_batch_1.csv file into a DataFrame called df_raw.\n",
    "# Display the .info() summary to show the initial messy state and wrong data types.\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "datafilepath = os.path.join(DATA_DIR, 'field_reports_batch_1.csv')\n",
    "\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "df_raw = None # placeholder\n",
    "\n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Perform Initial Diagnosis on Unclean Data\n",
    "\n",
    "STUDENT CODE REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of the different callsigns to see the typos using .value_counts() on the team_callsign column.\n",
    "\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "\n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Remove Corrupted Data from the Solar Storm Period\n",
    "\n",
    "STUDENT CODE REQUIRED\n",
    "\n",
    "To eliminate portions of corrupted data, use ```Pandas``` filtering commands to remove a section of time corresponding to the solar storm.\n",
    "In ```Pandas```, selecting rows *other than* a certain set can be accomplished using a combination of   ```~``` and\n",
    "boolean logic operators such as ```&``` using careful placement of parentheses to control the order of operations\n",
    "\n",
    "Additionally, the ```pandas``` operation ```.copy``` will return a copy of the datafram instead of altering the original\n",
    "\n",
    "Use ```pd.Timestamp``` to create filtering timestamps to exclude the solar storm period.  Timestamps are in the format ```'YYYY-MM-DD HH:MM:SS'```\n",
    "\n",
    "* use ```storm_start``` as ```'2025-08-21 08:00:00'```\n",
    "* and ```storm_end``` as ```'2025-08-21 09:30:00'```\n",
    "\n",
    "the original dataframe is ```df_raw``` and this code cell should create a filtered dataframe called ```df_filtered``` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'timestamp' column to a proper datetime object.\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'], errors='coerce')\n",
    "before_rows = len(df_raw)\n",
    "\n",
    "df_filtered = None  #placeholder for the filtered dataframe\n",
    "\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "\n",
    "# define the period of the solar storm to be exculded by defining the pd.Timestamp for storm_start and storm_end\n",
    "# the form of a timestamp is pd.Timestamp('YYYY-MM-DD HH:MM:SS')\n",
    "storm_start = None # placeholder\n",
    "storm_end = None # placeholder\n",
    "\n",
    "# Create a copy of a filtered DataFrame that excludes rows during the solar storm period \n",
    "# using boolean logic on the timestamps with ~ to exclude a selection and inequality operators such as >=  and <= along with & for boolean AND\n",
    "# make sure to copy the dataframe int df_filtered instead of altering df_raw\n",
    "df_filtered = None # placeholder\n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n",
    "\n",
    "\n",
    "after_rows = len(df_filtered)\n",
    "print(f\"Rows before filtering: {before_rows} (should be 200)\")\n",
    "print(f\"Rows after filtering: {after_rows} (should be 190)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Define the Reusable Cleaning Function\n",
    "\n",
    "STUDENT CODE REQUIRED\n",
    "\n",
    "Note:  After you run this cell during Step 5 and confirm it works properly in Step 6, the function you write here should be copied in the python file for utilities which can be imported later.  This file is called ```uav_analysis_tools.py``` and the importable function should be called ```clean_data```.  Don't change other parts of the ```uav_analysis_tools.py``` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clean_data function\n",
    "\n",
    "def clean_data(df, noise_floor=-120):\n",
    "    \"\"\"\n",
    "    Cleans the UAV field report DataFrame.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1.  Standardizes missing value placeholders ('?', 'N/A', '') to np.nan.\n",
    "    2.  Standardizes 'team_callsign' to lowercase, corrects known typos,\n",
    "        and fills any missing callsigns with 'unknown'.\n",
    "    3.  For the 'signal_strength' column, converts it to numeric (coercing errors)\n",
    "        and imputes missing values (NaNs) with the specified noise_floor.\n",
    "    4.  For all other numeric columns, converts them to numeric (coercing errors)\n",
    "        and imputes missing values with the column's median.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw field report DataFrame.\n",
    "        noise_floor (int or float, optional): The noise floor (in dBm) to use for\n",
    "            imputing missing signal strength values. Defaults to -120.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_clean = df.copy()   \n",
    "\n",
    "    ##########################################################\n",
    "    ##### START STUDENT CODE HERE:\n",
    "\n",
    "    # 1. Standardize missing values across the entire DataFrame using .replace()  you should covert all instances of  ['?', 'N/A', ''] to np.nan.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # 2. Handle categorical typos (team_callsign)\n",
    "    if 'team_callsign' in df_clean.columns:\n",
    "        \n",
    "        # Fill missing values, standardize casing and strip whitespace using .fillna(), .str.lower(), and .str.strip()\n",
    "        # Chain string operations for efficiency and clarity\n",
    "        df_clean['team_callsign'] = None # placeholder\n",
    "\n",
    "        # Correct known typos such as 'alfa' to 'alpha' by using a map of {`from`: `to`} and the .replace() method\n",
    "        typo_map = {'alfa': 'alpha'}\n",
    "        df_clean['team_callsign'] = None # placeholder\n",
    "\n",
    "\n",
    "    # 3. Special handling for signal_strength imputation\n",
    "    if 'signal_strength' in df_clean.columns:\n",
    "\n",
    "        # Convert to numeric, coercing errors (like 'ERR-&^%') to NaN by using pd.to_numeric() and the errors='coerce' parameter\n",
    "        df_clean['signal_strength'] = None # placeholder\n",
    "\n",
    "        # Impute NaN values with the noise_floor by using the dataframe .fillna() method, passing in the noise_floor parameter \n",
    "        df_clean['signal_strength'] = None # placeholder\n",
    "\n",
    "\n",
    "\n",
    "    # 4. Identify and impute other numeric columns with their median\n",
    "    # Exclude columns we've already handled or know are non-numeric using the dataframe .columns.drop() method and the errors='ignore' parameter\n",
    "    numeric_cols = df_clean.columns.drop(['timestamp', 'team_callsign', 'report_id', 'signal_strength'], errors='ignore')\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        # Convert to numeric, coercing errors to NaN using the pd.to_numeric() function with the errors='coerce' parameter\n",
    "        df_clean[col] = None # placeholder\n",
    "        # Calculate the median of the now-numeric column using the dataframe .median() method\n",
    "        median_val = None # placeholder\n",
    "        # Fill any NaNs (original or coerced) with the median using the dataframe .fillna() method with inplace=True\n",
    "        df_clean[col] = None # placeholder\n",
    "\n",
    "\n",
    "\n",
    "    ##### END STUDENT CODE HERE\n",
    "    ##########################################################\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Apply the Cleaning Function\n",
    "\n",
    "STUDENT CODE REQUIRED\n",
    "\n",
    "Special Note - after confirming the code you wrote for the clean_data() function works properly when it is called from this cell and gives the correct results in step 6, copy the function contents (from the signature line beginning with ```def``` all the way to the end of the ```return``` line) into the ```uav_analysis_tools.py``` file.  Don't change other parts of the ```uav_analysis_tools.py``` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from uav_analysis_tools import clean_data\n",
    "\n",
    "# Apply the clean_data function to the df_filtered DataFrame.\n",
    "# Store the result in df_clean and display the .info() summary.\n",
    "\n",
    "df_clean = None #placeholder for cleaned dataframe\n",
    "\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "\n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n",
    "\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Verify the Cleaning Results\n",
    "\n",
    "You should review the code and ensure that after running it on your ```df_clean``` dataframe, all tests are successful.  If any warnings appear it may be the case that the data was not cleaned properly\n",
    "\n",
    "Special Note - after confirming the code you wrote for the clean_data() function you wrote in Step 4 works properly when it is called from this cell, copy the function contents (from the signature line beginning with ```def``` all the way to the end of the ```return``` line) into the ```uav_analysis_tools.py``` file where indicated.  Don't change other parts of the ```uav_analysis_tools.py``` file\n",
    "\n",
    "No Student Code Required for this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Separate columns by their data type\n",
    "numeric_cols = df_clean.select_dtypes(include=np.number)\n",
    "categorical_cols = df_clean.select_dtypes(include='object')\n",
    "\n",
    "# 2. Verify the numeric columns\n",
    "print(\"--- Verification of Numeric Data ---\")\n",
    "print(\"This summary should show a count of 190 for all columns, with no NaNs or extreme outliers.\")\n",
    "display(numeric_cols.describe())\n",
    "\n",
    "\n",
    "#verify that the meaningful numeric columns contain reasonable values\n",
    "\n",
    "#verify that latitude is between -90 and 90 and that longitude is between -180 and 180\n",
    "if 'latitude' in numeric_cols.columns:\n",
    "    if ((numeric_cols['latitude'] < -90) | (numeric_cols['latitude'] > 90)).any():\n",
    "        print(\"Warning: Some latitude entries are out of the valid range (-90 to 90).\")\n",
    "    else:\n",
    "        print(\"All latitude entries are within the valid range (-90 to 90).\")  \n",
    "if 'longitude' in numeric_cols.columns:\n",
    "    if ((numeric_cols['longitude'] < -180) | (numeric_cols['longitude'] > 180)).any():\n",
    "        print(\"Warning: Some longitude entries are out of the valid range (-180 to 180).\")\n",
    "    else:\n",
    "        print(\"All longitude entries are within the valid range (-180 to 180).\")    \n",
    "\n",
    "#verify that wind direction is between 0 and 360\n",
    "if 'wind_direction_deg' in numeric_cols.columns:\n",
    "    if ((numeric_cols['wind_direction_deg'] < 0) | (numeric_cols['wind_direction_deg'] > 360)).any():\n",
    "        print(\"Warning: Some wind_direction_deg entries are out of the valid range (0-360).\")\n",
    "    else:\n",
    "        print(\"All wind_direction_deg entries are within the valid range (0-360).\")\n",
    "\n",
    "#verify that ambient_temperature is within a reasonable range in celcius\n",
    "if 'ambient_temperature' in numeric_cols.columns:\n",
    "    if ((numeric_cols['ambient_temperature'] < -50) | (numeric_cols['ambient_temperature'] > 60)).any():\n",
    "        print(\"Warning: Some ambient_temperature entries are outside the reasonable range (-50 to 60 °C).\")\n",
    "    else:\n",
    "        print(\"All ambient_temperature entries are within the reasonable range (-50 to 60 °C).\")\n",
    "\n",
    "#verify that battery_level_percent is between 0 and 100%\n",
    "if 'battery_level_percent' in numeric_cols.columns:\n",
    "    if ((numeric_cols['battery_level_percent'] < 0) | (numeric_cols['battery_level_percent'] > 100)).any():\n",
    "        print(\"Warning: Some battery_level_percent entries are out of the valid range (0-100%).\")\n",
    "    else:\n",
    "        print(\"All battery_level_percent entries are within the valid range (0-100%).\")\n",
    "\n",
    "#verify that signal_strength has no values below the noise floor\n",
    "if 'signal_strength' in numeric_cols.columns:\n",
    "    if (numeric_cols['signal_strength'] < -120).any():\n",
    "        print(\"Warning: Some signal_strength entries are below the noise floor (-120 dBm).\")\n",
    "    else:\n",
    "        print(\"All signal_strength entries are above the noise floor (-120 dBm).\")  \n",
    "\n",
    "print(\"\\n\" * 2) # Add space for readability\n",
    "\n",
    "# 3. Verify the categorical columns\n",
    "print(\"--- Verification of Categorical Data ---\")\n",
    "print(\"This summary should show standardized, lowercase callsigns.\")\n",
    "display(categorical_cols.describe())\n",
    "#show the top 10 most common callsigns to verify no typos\n",
    "if 'team_callsign' in categorical_cols.columns:\n",
    "    print(\"\\nTop 10 most common team_callsign values:\")\n",
    "    print(categorical_cols['team_callsign'].value_counts().head(10))    \n",
    "\n",
    "# 4. Additional checks for specific columns\n",
    "# check that all team_callsign values are in lowercase and contain no leading/trailing whitespace\n",
    "if 'team_callsign' in categorical_cols.columns:\n",
    "    callsign_issues = categorical_cols['team_callsign'].apply(lambda x: x != x.strip() or not x.islower())\n",
    "    if callsign_issues.any():\n",
    "        print(\"Warning: Some team_callsign entries have casing or whitespace issues.\")\n",
    "    else:\n",
    "        #count the number of unique callsigns\n",
    "        unique_callsigns = categorical_cols['team_callsign'].nunique()\n",
    "        print(f\"Number of unique team_callsign entries: {unique_callsigns}\")\n",
    "        print(\"All team_callsign entries are properly formatted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Save the Final Cleaned Dataset\n",
    "\n",
    "No student code required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_clean to a new CSV file named cleaned_reports_batch_1.csv in the data subdierctory.\n",
    "# Do not include the pandas index in the saved file.\n",
    "# Print a confirmation message.\n",
    "output_path = os.path.join(DATA_DIR, 'cleaned_reports_batch_1.csv')\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"File '{output_path}' has been saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
