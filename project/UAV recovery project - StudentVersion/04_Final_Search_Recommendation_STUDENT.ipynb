{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f048b81a",
   "metadata": {},
   "source": [
    "# Part 4: Feature Engineering and Data Integration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Scenario\n",
    "\n",
    "Your initial assessment has successfully identified a high-priority search area. Command has redirected search teams to this new zone, and a second batch of field reports has just arrived. Your mission is to integrate this new data with the first batch and hopefully narrow the search location even more. \n",
    "\n",
    "### Coding Task Overview\n",
    "\n",
    "You will import both of your functions from your utility file. Your task is to load and clean the second batch of data, then combine it with the cleaned data from the first batch. Finally, you will re-run your EDA function on this final, augmented dataset to produce an updated assesment of the likely focused region to narrow the search.\n",
    "\n",
    "### Mission Deliverable\n",
    "\n",
    "An Updated Assessment Briefing. You must produce the visualizations from your EDA function on the combined dataset and write a markdown summary defining a tight search region and describe the evidence used to support your assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0952ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas, numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# From our utility file, import both the clean_data and perform_eda functions\n",
    "from uav_analysis_tools import clean_data, perform_eda \n",
    "from uav_mapping_tools import EDA_makeMap  \n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8dd89",
   "metadata": {},
   "source": [
    "### STEP 1: Load All Datasets\n",
    "\n",
    "STUDENT CODE REQUIRED\n",
    "\n",
    "In this step you will \n",
    "1.  use pandas ```read.csv()``` to load your previously cleaned dataset CSV into a dataframe ```df_batch1```\n",
    "2.  use pandas ```read.csv()``` toload the new data ```field_reports_batch_2.csv``` into a dataframe ```df_batch2_raw```\n",
    "3.  use pandas ```.info()``` method to get an overview of the new raw data\n",
    "\n",
    "remember that these data files are stored in the ```data``` subdierctory so you will need the ```os``` \n",
    "package's ```os.path.join()``` function to create the proper load location for pandas' CSV reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = None #placeholder for the data directory\n",
    "datafilepath = None #placeholder for the data file path \n",
    "datafilepath2 = None #placeholder for the second data file path\n",
    "\n",
    "df_batch1 = None # placeholder\n",
    "df_batch2_raw = None  # placeholder\n",
    "\n",
    "# Load the cleaned_reports_batch_1.csv file into a DataFrame called df_batch1.\n",
    "# Load the new messy field_reports_batch_2.csv file into a DataFrame called df_batch2_raw.\n",
    "# Display the .info() for the new raw data to see its messy state.\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "\n",
    "#Set the DATA_DIR variable to 'data'\n",
    "DATA_DIR = None # placeholder\n",
    "\n",
    "#create the datafilepath variable by joining DATA_DIR and 'cleaned_reports_batch_1.csv' using os.path.join()\n",
    "datafilepath = None # placeholder\n",
    "df_batch1 = None # placeholder\n",
    "\n",
    "datafilepath2 = None # placeholder\n",
    "df_batch2_raw = None # placeholder\n",
    "\n",
    "#display the .info() for the new raw data to see its messy state.\n",
    " \n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42916504",
   "metadata": {},
   "source": [
    "### STEP 2: Clean the New Data Batch\n",
    "\n",
    "STUDENT CODE REQUIRED\n",
    "\n",
    "```uav_analysis_tools.py``` was imported earlier in this notebook.  Run the ```clean_data``` function from the ```uav_analysis_tools.py``` that you wrote previously \n",
    "and store the result in a dataframe called ```df_batch2_clean```.   Then use pandas ```.info()``` method to get the info on the cleaned dataset so that\n",
    "you can compare the cleaned and raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your imported clean_data function to clean the df_batch2_raw DataFrame.\n",
    "# Store the result in a new DataFrame called df_batch2_clean.\n",
    "# Display the .info() for the newly cleaned data to verify the result.\n",
    "\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "df_batch2_clean = None # placeholder\n",
    "\n",
    "#display the .info() for the newly cleaned data to verify the result.\n",
    "\n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1049bc",
   "metadata": {},
   "source": [
    "### STEP 3: Combine the Datasets\n",
    "\n",
    "STUDENT CODE REQUIRED\n",
    "\n",
    "Use ```pd.concat()``` to combine ```df_batch1``` and ```df_batch2_clean``` into a single DataFrame called ```df_combined```.\n",
    "Be sure to ignore the index to create a new, clean index using ```ignore_index=True```.\n",
    "Print the shape of all three DataFrames to confirm the concatenation was successful using the DataFrame's built in ```.shape``` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pd.concat to combine df_batch1 and df_batch2_clean into a single DataFrame\n",
    "# called df_combined.\n",
    "# Be sure to ignore the index to create a new, clean index.\n",
    "# Print the shape of all three DataFrames to confirm the concatenation was successful.\n",
    "df_combined = pd.concat([df_batch1, df_batch2_clean], ignore_index=True)\n",
    "print(f\"Batch 1 shape: {df_batch1.shape}\")\n",
    "print(f\"Batch 2 shape: {df_batch2_clean.shape}\")\n",
    "print(f\"Combined shape: {df_combined.shape}\")   \n",
    "\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "df_combined = None # placeholder\n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n",
    "\n",
    "#check to see that the combined data's shape is the sum of the two individual dataframes and print the result of whether the check succeeds or not\n",
    "expected_rows = df_batch1.shape[0] + df_batch2_clean.shape[0]\n",
    "actual_rows = df_combined.shape[0]\n",
    "if expected_rows == actual_rows:\n",
    "    print(\"Concatenation successful: row counts match.\")\n",
    "else:\n",
    "    print(\"Concatenation error: row counts do not match.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba82a7",
   "metadata": {},
   "source": [
    "### STEP 5: Map the locations of the reports in the engineered dataset\n",
    "\n",
    "No Student Code Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the map visualizations using the EDA_makeMap function on the df_engineered DataFrame\n",
    "\n",
    "m = EDA_makeMap(df_combined,force_regenerate=True)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57d2b9",
   "metadata": {},
   "source": [
    "### STEP 5b: Run Exploratory Data Analysis (EDA) on Engineered Dataset\n",
    "\n",
    "STUDENT CODE REQUIRED\n",
    "\n",
    "In this step you will apply your imported ```perform_eda()``` function to the final ```df_engineered``` DataFrame.\n",
    "The target column is still 'signal_strength'\n",
    "\n",
    "The function should automatically detect and plot the new engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your imported perform_eda function to the final df_engineered DataFrame.\n",
    "# The target column is still 'signal_strength'.\n",
    "# The function should automatically detect and plot the new engineered features.\n",
    "\n",
    "##########################################################\n",
    "##### START STUDENT CODE HERE:\n",
    "\n",
    "perform_eda(df_combined, 'signal_strength')   \n",
    "\n",
    "##### END STUDENT CODE HERE\n",
    "##########################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f22d9",
   "metadata": {},
   "source": [
    "### STEP 6: Mission Deliverable - Updated Assessment Briefing\n",
    "\n",
    "STUDENT MARKDOWN REQUIRED\n",
    "\n",
    "In the Markdown cell below write a short mission assessment where you provide \n",
    "1.  An overview of the assessment process\n",
    "2.  Evidence for your assessment based on the map and other EDA information\n",
    "3.  A bounding box that includes your guess about where the rescue beacon is located (lat/long coordinates of two opposing corners of the box)\n",
    "\n",
    "\n",
    "\n",
    "Note that clicking on the interactive map from Step 5a will provide lat/long coordinates that can be used to define the bounding box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266ddd6",
   "metadata": {},
   "source": [
    "# Updated Assessment Briefing\n",
    "\n",
    "**TO:** Mission Command\n",
    "**FROM:** Data Analysis Unit\n",
    "**SUBJECT:** Updated Search Area Recommendation Based on Integrated Field Data (Batch 1 & 2)\n",
    "\n",
    "### 1. Assessment Overview\n",
    "\n",
    "<span style=\"color: green;\">\n",
    "\n",
    "STUDENT TEXT HERE\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "### 2. Evidence\n",
    "\n",
    "<span style=\"color: green;\">\n",
    "\n",
    "STUDENT TEXT HERE\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "### 3. Final Recommendation for refined search area\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color: green;\">\n",
    "\n",
    "STUDENT TEXT HERE\n",
    "\n",
    "</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc4256",
   "metadata": {},
   "source": [
    "### STEP 7: Save the Final Combined Dataset\n",
    "\n",
    "No Student Code Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f87423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df_engineered DataFrame to a new CSV file named \n",
    "# 'final_combined_data.csv'.\n",
    "# Do not include the pandas index.\n",
    "# Print a confirmation message.\n",
    "\n",
    "\n",
    "#Set the DATA_DIR variable to 'data'\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "#create the datafilepath variable by joining DATA_DIR and 'cleaned_reports_batch_1.csv' using os.path.join()\n",
    "datafilepath = os.path.join(DATA_DIR, 'final_combined_data.csv')\n",
    "df_combined.to_csv(datafilepath, index=False)\n",
    "\n",
    "print(\"Final combined data saved to \", datafilepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
